---
apiVersion: v1
kind: Namespace
metadata:
  name: urbackup
  labels:
    name: urbackup

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: urbackup
  name: urbackup
  namespace: urbackup
spec:
  replicas: 1
  selector:
    matchLabels:
      app: urbackup
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: urbackup
    spec:
      containers:
        - env:
            - name: PGID
              value: "1000"
            - name: PUID
              value: "1000"
            - name: TZ
              value: America/Chicago
          image: uroni/urbackup-server:latest
          name: urbackup
          ports:
            - containerPort: 55414
              name: urbackup-tcp         # < reference name for the port in the deployment yaml
              protocol: TCP
          resources: {}
          volumeMounts:
            - mountPath: /var/urbackup
              name: urbackup-claim-db
            - mountPath: /backups
              name: urbackup-claim-data
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
        - name: urbackup-claim-db
          persistentVolumeClaim:
            claimName: urbackup-pvc-nfs-db
        - name: urbackup-claim-data
          persistentVolumeClaim:
            claimName: urbackup-pvc-nfs-data

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: urbackup-pv-nfs-db   # < name of the persisant volume ("pv") in kubenetes
  namespace: urbackup            # < namespace where place the pv
spec:
  storageClassName: ""
  capacity:
    storage: 4Gi                   # < max. size we reserve for the pv
  accessModes:
    - ReadWriteMany                # < Multiple pods can write to storage 
  persistentVolumeReclaimPolicy: Retain # < The persistent volume can reclaimed 
  nfs:
    path: /vm-tank/k3s/urbackup        # < Name of your NFS share with subfolder
    server: 192.168.1.11      # < IP number of your NFS server
    readOnly: false

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: urbackup-pvc-nfs-db
  namespace: urbackup
spec:
  storageClassName: ""
  volumeName: urbackup-pv-nfs-db
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 4Gi

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: urbackup-pv-nfs-data   # < name of the persisant volume ("pv") in kubenetes
  namespace: urbackup            # < namespace where place the pv
spec:
  storageClassName: ""
  capacity:
    storage: 100Gi                   # < max. size we reserve for the pv
  accessModes:
    - ReadWriteMany                # < Multiple pods can write to storage 
  persistentVolumeReclaimPolicy: Retain # < The persistent volume can reclaimed 
  nfs:
    path: /cpool1/k3s/urbackup        # < Name of your NFS share with subfolder
    server: 192.168.1.10      # < IP number of your NFS server
    readOnly: false

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: urbackup-pvc-nfs-data
  namespace: urbackup
spec:
  storageClassName: ""
  volumeName: urbackup-pv-nfs-data
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Gi

---
kind: Service
apiVersion: v1
metadata:
  name: urbackup-lb       # < name of the service
  namespace: urbackup       # < namespace where to place service
  annotations:
    metallb.universe.tf/allow-shared-ip: urbackup # < annotation name to combine the Service IP, make sure it's same name as in the service UDP yaml
spec:
  selector:
    app: urbackup          # < reference to the deployment (connects the service with the deployment)
  ports:
  - port: 55414             # < port to open on the outside on the server
    targetPort: 55414       # < targetport. port on the pod to passthrough
    name: urbackup-tcp         # < reference name for the port in the deployment yaml
    protocol: TCP
  type: LoadBalancer
  loadBalancerIP: 192.168.3.83 # < IP to access your jellyfinserver. Should be one from the MetalLB range and the same as the TCP yaml
  sessionAffinity: ClientIP # This is necessary for multi-replica deployments

