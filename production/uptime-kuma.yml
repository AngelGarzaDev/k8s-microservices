---
apiVersion: v1
kind: Namespace
metadata:
  name: uptime-kuma
  labels:
    name: uptime-kuma

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: uptime-kuma
  namespace: uptime-kuma
spec:
  replicas: 1
  selector:
    matchLabels:
      app: uptime-kuma
  template:
    metadata:
      labels:
        app: uptime-kuma
    spec:
      # Mount the PVC
      volumes:
        - name: data-mount
          persistentVolumeClaim:
            claimName: uptime-data-claim
      containers:
        - name: uptime-kuma
          image: louislam/uptime-kuma:latest
          # expose the container's port
          ports:
            - containerPort: 3001
          # Pass the mount into the container
          volumeMounts:
            - mountPath: '/app/data'
              name: data-mount
          # optional but recommended to set usage limits
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 128Mi
          imagePullPolicy: IfNotPresent

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: uptime-data
  namespace: uptime-kuma
  labels:
    type: local
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: ""
  nfs:
    path: /mnt/servicesTank/uptimekuma       # < Name of your NFS share with subfolder
    server: 192.168.1.10      # < IP number of your NFS server
    readOnly: false

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: uptime-data-claim
  namespace: uptime-kuma
spec:
  storageClassName: ""
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

---
kind: Service
apiVersion: v1
metadata:
  name: uptime-kuma-tcp       # < name of the service
  namespace: uptime-kuma      # < namespace where to place service
  annotations:
    metallb.universe.tf/allow-shared-ip: uptime-kuma # < annotation name to combine the Service IP, make sure it's same name as in the service UDP yaml
spec:
  selector:
    app: uptime-kuma          # < reference to the deployment (connects the service with the deployment)
  ports:
  - port: 80             # < port to open on the outside on the server
    targetPort: 3001       # < targetport. port on the pod to passthrough
    name: http-tcp-kuma         # < reference name for the port in the deployment yaml
    protocol: TCP
  type: LoadBalancer
  loadBalancerIP: 192.168.3.82 # < IP to access your uptime-kumaserver. Should be one from the MetalLB range and the same as the TCP yaml
  sessionAffinity: ClientIP # This is necessary for multi-replica deployments
